import os
from typing import List, Dict, Optional, Literal

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

import chromadb
from chromadb.config import Settings
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction

# =========================
# Config
# =========================
DATA_DIR = os.getenv("DATA_DIR", "data/raw")
PERSIST_DIR = os.getenv("PERSIST_DIR", "vectorstore")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "kb")

# Embeddings (–ª–æ–∫–∞–ª—å–Ω–æ, –±–µ–∑ –∫–ª—é—á—ñ–≤)
EMBED_MODEL = os.getenv("EMBED_MODEL", "all-MiniLM-L6-v2")

# LLM provider
# - openai: –æ—Ñ—ñ—Ü—ñ–π–Ω–∏–π OpenAI
# - compatible: OpenAI-compatible (DeepSeek, —ñ–Ω—à—ñ)
# - gemini: Google Gemini (–æ–ø—Ü—ñ–π–Ω–æ; —Ç—Ä–µ–±–∞ –¥–æ–¥. –ø–∞–∫–µ—Ç)
PROVIDER: Literal["openai", "compatible", "gemini"] = os.getenv("PROVIDER", "openai")  # openai|compatible|gemini
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")

# OpenAI-compatible settings
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "")  # –¥–ª—è compatible: –Ω–∞–ø—Ä. https://api.deepseek.com

# Gemini settings (–æ–ø—Ü—ñ–π–Ω–æ)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")

# RAG behavior
DEFAULT_TOP_K = int(os.getenv("TOP_K", "6"))
MAX_TOP_K = 12

SYSTEM_STYLE = os.getenv(
    "SYSTEM_STYLE",
    "–¢–∏ –¥—Ä—É–∂–Ω—ñ–π –Ω–∞–≤—á–∞–ª—å–Ω–∏–π –∞—Å–∏—Å—Ç–µ–Ω—Ç –∑ –±—ñ–æ–ª–æ–≥—ñ—ó. "
    "–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é, –ø—Ä–æ—Å—Ç–æ –π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ. "
    "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –ö–û–ù–¢–ï–ö–°–¢ —è–∫ –¥–∂–µ—Ä–µ–ª–æ, –∞–ª–µ –ù–ï —Ü–∏—Ç—É–π –π–æ–≥–æ –¥–æ—Å–ª—ñ–≤–Ω–æ —ñ –ù–ï –ø–æ–∫–∞–∑—É–π —É—Ä–∏–≤–∫–∏. "
    "–Ø–∫—â–æ —É –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ –Ω–µ–º–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ ‚Äî —Å–∫–∞–∂–∏ —á–µ—Å–Ω–æ —ñ –ø–æ–ø—Ä–æ—Å–∏ —É—Ç–æ—á–Ω–µ–Ω–Ω—è."
)

# =========================
# App
# =========================
app = FastAPI(title="BioConsult RAG Server")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω—É –∫—Ä–∞—â–µ –∑–≤—É–∑–∏—Ç–∏
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =========================
# Chroma (persistent)
# =========================
chroma = chromadb.PersistentClient(
    path=PERSIST_DIR,
    settings=Settings(anonymized_telemetry=False)
)

embedding_fn = SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL)


def get_collection():
    return chroma.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_fn
    )


# =========================
# Utils
# =========================
def list_raw_files() -> List[str]:
    if not os.path.isdir(DATA_DIR):
        return []
    out = []
    for fn in os.listdir(DATA_DIR):
        if fn.lower().endswith((".txt", ".md")):
            out.append(os.path.join(DATA_DIR, fn))
    return sorted(out)


def read_text(path: str) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()


def chunk_text(text: str, chunk_size: int = 900, overlap: int = 120) -> List[str]:
    clean = " ".join((text or "").split())
    if not clean:
        return []
    out = []
    i = 0
    while i < len(clean):
        end = min(len(clean), i + chunk_size)
        out.append(clean[i:end])
        i = max(0, end - overlap)
        if end == len(clean):
            break
    return out


def rebuild_index() -> Dict:
    files = list_raw_files()
    if not files:
        return {"ok": True, "files": 0, "chunks": 0, "note": f"–ù–µ–º–∞—î .txt/.md —É {DATA_DIR}"}

    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—î–º–æ –∫–æ–ª–µ–∫—Ü—ñ—é (–ø—Ä–æ—Å—Ç–∏–π —ñ –Ω–∞–¥—ñ–π–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç)
    try:
        chroma.delete_collection(COLLECTION_NAME)
    except Exception:
        pass

    col = get_collection()

    ids: List[str] = []
    docs: List[str] = []
    metas: List[Dict] = []

    for path in files:
        title = os.path.basename(path)
        text = read_text(path)
        parts = chunk_text(text)
        for idx, p in enumerate(parts):
            ids.append(f"{title}#{idx}")
            docs.append(p)
            metas.append({"title": title, "chunk": idx})

    if not docs:
        return {"ok": True, "files": len(files), "chunks": 0}

    # add without embeddings (embedding_function –∑—Ä–æ–±–∏—Ç—å —ó—Ö —Å–∞–º–∞)
    col.add(ids=ids, documents=docs, metadatas=metas)

    return {"ok": True, "files": len(files), "chunks": len(docs)}


def retrieve(question: str, k: int = DEFAULT_TOP_K) -> List[Dict]:
    col = get_collection()
    k = max(1, min(int(k), MAX_TOP_K))

    res = col.query(
        query_texts=[question],
        n_results=k
    )

    out = []
    documents = (res.get("documents") or [[]])[0]
    metadatas = (res.get("metadatas") or [[]])[0]

    for doc, meta in zip(documents, metadatas):
        out.append({
            "title": (meta or {}).get("title", "kb"),
            "text": doc
        })
    return out


def build_messages(question: str, contexts: List[Dict], history: List[Dict]) -> List[Dict]:
    # history: [{"role":"user"|"assistant","content":"..."}]
    context_block = "\n\n".join(
        [f"[{i+1}] ({c['title']}) {c['text']}" for i, c in enumerate(contexts)]
    )

    user_prompt = (
        f"–ö–û–ù–¢–ï–ö–°–¢ (–¥–ª—è —Ç–µ–±–µ):\n{context_block}\n\n"
        f"–ü–ò–¢–ê–ù–ù–Ø: {question}\n\n"
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å: –∫–æ—Ä–æ—Ç–∫–æ, —è—Å–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ. "
        "–ù–µ —Ü–∏—Ç—É–π —É—Ä–∏–≤–∫–∏ –∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–æ—Å–ª—ñ–≤–Ω–æ."
    )

    msgs = [{"role": "system", "content": SYSTEM_STYLE}]

    # –¥–æ–¥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ 8 —Ä–µ–ø–ª—ñ–∫ —ñ—Å—Ç–æ—Ä—ñ—ó (—â–æ–± –Ω–µ —Ä–æ–∑–¥—É–≤–∞—Ç–∏)
    if history:
        trimmed = history[-8:]
        for m in trimmed:
            role = m.get("role")
            content = m.get("content", "")
            if role in ("user", "assistant") and content:
                msgs.append({"role": role, "content": content})

    msgs.append({"role": "user", "content": user_prompt})
    return msgs


# =========================
# LLM call
# =========================
def llm_answer(messages: List[Dict]) -> str:
    if PROVIDER in ("openai", "compatible"):
        if not OPENAI_API_KEY:
            raise HTTPException(status_code=400, detail="–ù–µ–º–∞—î OPENAI_API_KEY —É –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞.")

        from openai import OpenAI

        if PROVIDER == "compatible":
            if not OPENAI_BASE_URL:
                raise HTTPException(status_code=400, detail="–î–ª—è PROVIDER=compatible –≤–∫–∞–∂–∏ OPENAI_BASE_URL.")
            cli = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
        else:
            cli = OpenAI(api_key=OPENAI_API_KEY)

        resp = cli.chat.completions.create(
            model=CHAT_MODEL,
            messages=messages,
            temperature=0.4
        )
        return (resp.choices[0].message.content or "").strip()

    if PROVIDER == "gemini":
        # –û–ø—Ü—ñ–π–Ω–æ: –≤—Å—Ç–∞–Ω–æ–≤–∏ –ø–∞–∫–µ—Ç google-genai
        # pip install google-genai
        if not GEMINI_API_KEY:
            raise HTTPException(status_code=400, detail="–ù–µ–º–∞—î GEMINI_API_KEY —É –∑–º—ñ–Ω–Ω–∏—Ö —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞.")
        try:
            from google import genai
        except Exception:
            raise HTTPException(
                status_code=400,
                detail="–î–ª—è Gemini —Ç—Ä–µ–±–∞ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ –ø–∞–∫–µ—Ç: pip install google-genai"
            )

        client = genai.Client(api_key=GEMINI_API_KEY)

        # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ messages —É —Ç–µ–∫—Å—Ç (–ø—Ä–æ—Å—Ç–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç)
        joined = ""
        for m in messages:
            joined += f"{m['role'].upper()}: {m['content']}\n\n"

        resp = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=joined
        )
        return (resp.text or "").strip()

    raise HTTPException(status_code=400, detail="–ù–µ–≤—ñ–¥–æ–º–∏–π PROVIDER.")


# =========================
# API —Å—Ö–µ–º–∏
# =========================
class ChatIn(BaseModel):
    question: str
    rag: bool = True
    top_k: int = DEFAULT_TOP_K
    history: List[Dict] = []  # [{"role":"user"/"assistant","content":"..."}]


class ChatOut(BaseModel):
    answer: str
    used_contexts: int


# =========================
# API routes
# =========================
@app.get("/api/health")
def health():
    return {"ok": True, "provider": PROVIDER, "model": CHAT_MODEL, "embed": EMBED_MODEL}


@app.post("/api/reindex")
def reindex():
    return rebuild_index()


@app.post("/api/chat", response_model=ChatOut)
def chat(payload: ChatIn):
    q = (payload.question or "").strip()
    if not q:
        return ChatOut(answer="–ù–∞–ø–∏—à–∏ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è üôÇ", used_contexts=0)

    contexts = retrieve(q, payload.top_k) if payload.rag else []
    messages = build_messages(q, contexts, payload.history)

    answer = llm_answer(messages)
    if not answer:
        answer = "–Ø –Ω–µ –∑–º—ñ–≥ —Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å. –°–ø—Ä–æ–±—É–π –ø–µ—Ä–µ—Ñ—Ä–∞–∑—É–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è."

    return ChatOut(answer=answer, used_contexts=len(contexts))


# =========================
# Serve frontend (app/)
# =========================
# –í–∞–∂–ª–∏–≤–æ: –º–æ–Ω—Ç—É—î–º–æ –ø—ñ—Å–ª—è /api, —â–æ–± /api/* –ø—Ä–∞—Ü—é–≤–∞–ª–æ
if os.path.isdir("app"):
    app.mount("/", StaticFiles(directory="app", html=True), name="static")
