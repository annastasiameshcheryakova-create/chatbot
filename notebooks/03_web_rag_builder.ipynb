{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c03416",
   "metadata": {},
   "source": [
    "# BioConsult: Web-збір + аналіз + експорт у data/raw\n",
    "\n",
    "Цей ноутбук:\n",
    "- шукає інформацію в Wikipedia, PubMed, OpenAlex, Crossref\n",
    "- завантажує сторінки та витягує основний текст\n",
    "- формує просту «нормальну» відповідь без показу конспектів\n",
    "- експортує файли у `../data/raw/` для твого GitHub Pages проєкту\n",
    "\n",
    "> Запусти клітинки по черзі. В кінці зміни `query` та перезапусти останній блок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install requests beautifulsoup4 trafilatura lxml tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1142b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, json, textwrap, time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import trafilatura\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"BioConsultStudyBot/1.0 (educational)\"\n",
    "}\n",
    "\n",
    "# Якщо ноутбук лежить у /notebooks, то DATA_DIR = ../data/raw\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = re.sub(r\"\\s+\", \" \", t or \"\").strip()\n",
    "    return t\n",
    "\n",
    "def truncate(t: str, n=3500) -> str:\n",
    "    return t[:n] + (\"…\" if len(t) > n else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574bb64a",
   "metadata": {},
   "source": [
    "## Пошук по джерелах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_wikipedia(query: str, lang: str = \"uk\", limit: int = 5) -> List[Dict]:\n",
    "    url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": query,\n",
    "        \"format\": \"json\",\n",
    "        \"utf8\": 1\n",
    "    }\n",
    "    r = requests.get(url, params=params, headers=HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    out = []\n",
    "    for item in data.get(\"query\", {}).get(\"search\", [])[:limit]:\n",
    "        title = item[\"title\"]\n",
    "        page_url = f\"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        out.append({\"source\": \"wikipedia\", \"title\": title, \"url\": page_url})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022fc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_pubmed(query: str, limit: int = 5) -> List[Dict]:\n",
    "    esearch = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    r = requests.get(esearch, params={\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": query,\n",
    "        \"retmax\": limit,\n",
    "        \"retmode\": \"json\"\n",
    "    }, headers=HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    ids = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "    out = []\n",
    "    for pmid in ids:\n",
    "        out.append({\"source\": \"pubmed\", \"title\": f\"PubMed PMID:{pmid}\", \"url\": f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505592db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_openalex(query: str, limit: int = 5) -> List[Dict]:\n",
    "    url = \"https://api.openalex.org/works\"\n",
    "    r = requests.get(url, params={\"search\": query, \"per_page\": limit}, headers=HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    out = []\n",
    "    for w in data.get(\"results\", [])[:limit]:\n",
    "        title = w.get(\"title\") or \"Untitled\"\n",
    "        landing = w.get(\"primary_location\", {}).get(\"landing_page_url\") or w.get(\"id\")\n",
    "        out.append({\"source\": \"openalex\", \"title\": title, \"url\": landing})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_crossref(query: str, limit: int = 5) -> List[Dict]:\n",
    "    url = \"https://api.crossref.org/works\"\n",
    "    r = requests.get(url, params={\"query\": query, \"rows\": limit}, headers=HEADERS, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    items = r.json().get(\"message\", {}).get(\"items\", [])[:limit]\n",
    "    out = []\n",
    "    for it in items:\n",
    "        title = (it.get(\"title\") or [\"Untitled\"])[0]\n",
    "        link = it.get(\"URL\")\n",
    "        if not link:\n",
    "            continue\n",
    "        out.append({\"source\": \"crossref\", \"title\": title, \"url\": link})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab174e3b",
   "metadata": {},
   "source": [
    "## Завантаження і витяг тексту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a869bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_and_extract(url: str) -> str:\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if not downloaded:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=25)\n",
    "            r.raise_for_status()\n",
    "            downloaded = r.text\n",
    "        text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n",
    "        return clean_text(text or \"\")\n",
    "    except Exception:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Doc:\n",
    "    title: str\n",
    "    url: str\n",
    "    source: str\n",
    "    text: str\n",
    "\n",
    "def build_corpus(query: str, lang=\"uk\", per_source=3) -> List[Doc]:\n",
    "    hits = []\n",
    "    hits += search_wikipedia(query, lang=lang, limit=per_source)\n",
    "    hits += search_pubmed(query, limit=per_source)\n",
    "    hits += search_openalex(query, limit=per_source)\n",
    "    hits += search_crossref(query, limit=per_source)\n",
    "\n",
    "    docs: List[Doc] = []\n",
    "    for h in hits:\n",
    "        text = fetch_and_extract(h[\"url\"])\n",
    "        if len(text) < 500:\n",
    "            continue\n",
    "        docs.append(Doc(title=h[\"title\"], url=h[\"url\"], source=h[\"source\"], text=text))\n",
    "        time.sleep(0.25)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b001e",
   "metadata": {},
   "source": [
    "## Простий «офлайн AI»-конспект без показу фрагментів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_answer(query: str, docs: List[Doc], max_chars=3500) -> str:\n",
    "    if not docs:\n",
    "        return \"Не вдалося знайти достатньо відкритих джерел під цей запит.\"\n",
    "\n",
    "    blocks = []\n",
    "    for d in docs[:6]:\n",
    "        snippet = truncate(d.text, 900)\n",
    "        blocks.append(snippet)\n",
    "\n",
    "    merged = \" \".join(blocks)\n",
    "    merged = truncate(merged, max_chars)\n",
    "\n",
    "    answer = []\n",
    "    answer.append(f\"Запит: {query}\\n\")\n",
    "    answer.append(\"Коротко по суті:\")\n",
    "    answer.append(textwrap.fill(merged, width=100))\n",
    "    answer.append(\"\\nЯкщо хочеш — напиши формат: (1) визначення, (2) етапи, (3) приклади, (4) порівняння — і я перероблю.\")\n",
    "    return \"\\n\\n\".join(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222771bb",
   "metadata": {},
   "source": [
    "## Експорт у `../data/raw/` для GitHub Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_to_txt(docs: List[Doc], out_path: str):\n",
    "    lines = []\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        lines.append(f\"=== DOC #{i} ===\")\n",
    "        lines.append(f\"TITLE: {d.title}\")\n",
    "        lines.append(f\"SOURCE: {d.source}\")\n",
    "        lines.append(f\"URL: {d.url}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(d.text)\n",
    "        lines.append(\"\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "def export_answer_only(answer: str, out_path: str):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce44f8",
   "metadata": {},
   "source": [
    "## Запуск\n",
    "\n",
    "Зміни `query` і запусти. Файли збережуться у:\n",
    "- `data/raw/web_notes.txt`\n",
    "- `data/raw/web_answer.txt`\n",
    "\n",
    "Потім зроби **commit+push**, щоб GitHub Pages їх підхопив."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f0793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"мітохондрія функції етапи утворення АТФ\"  # <-- змінюй тут\n",
    "\n",
    "docs = build_corpus(query, lang=\"uk\", per_source=3)\n",
    "print(\"Docs:\", len(docs))\n",
    "\n",
    "out_corpus = os.path.join(DATA_DIR, \"web_notes.txt\")\n",
    "export_to_txt(docs, out_corpus)\n",
    "print(\"Saved:\", out_corpus)\n",
    "\n",
    "ans = simple_answer(query, docs)\n",
    "out_ans = os.path.join(DATA_DIR, \"web_answer.txt\")\n",
    "export_answer_only(ans, out_ans)\n",
    "print(\"Saved:\", out_ans)\n",
    "\n",
    "print(\"\\n--- ANSWER PREVIEW ---\\n\")\n",
    "print(ans[:1200])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
