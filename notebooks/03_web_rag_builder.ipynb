{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9c03416",
      "metadata": {},
      "source": [
        "# BioConsult: Web-збір + аналіз + експорт у data/raw (extended)\n",
        "\n",
        "Цей ноутбук:\n",
        "- шукає інформацію в Wikipedia, PubMed, OpenAlex, Crossref\n",
        "- завантажує сторінки та витягує основний текст\n",
        "- робить **нормальний короткий конспект** (без показу сирих конспектів)\n",
        "- **чистить сміття типу `=====`**\n",
        "- формує **групи запитів** у форматі `ГРУПА ЗАПИТУ / Ключові слова / Відповідь` (під твій веб-бот)\n",
        "- експортує файли у `../data/raw/`\n",
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db5767f",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install requests beautifulsoup4 trafilatura lxml tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1142b61",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, json, textwrap, time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import trafilatura\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"BioConsultStudyBot/1.0 (educational)\"\n",
        "}\n",
        "\n",
        "# Якщо ноутбук лежить у /notebooks, то DATA_DIR = ../data/raw\n",
        "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "SEP_RE = re.compile(r\"^(={3,}|-{3,}|_{3,}|\\*{3,})\\s*$\", re.MULTILINE)\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    t = (t or \"\")\n",
        "    # прибираємо сміттєві розділювачі типу =====\n",
        "    t = SEP_RE.sub(\"\", t)\n",
        "    # прибираємо надлишкові пробіли\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def truncate(t: str, n=3500) -> str:\n",
        "    t = t or \"\"\n",
        "    return t[:n] + (\"…\" if len(t) > n else \"\")\n",
        "\n",
        "def safe_filename(name: str) -> str:\n",
        "    name = (name or \"file\").strip().lower()\n",
        "    name = re.sub(r\"[^a-z0-9а-яіїєґ_\\- ]+\", \"\", name)\n",
        "    name = re.sub(r\"\\s+\", \"_\", name)\n",
        "    return name[:80] or \"file\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "574bb64a",
      "metadata": {},
      "source": [
        "## Пошук по джерелах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cd4d8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_wikipedia(query: str, lang: str = \"uk\", limit: int = 5) -> List[Dict]:\n",
        "    url = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"list\": \"search\",\n",
        "        \"srsearch\": query,\n",
        "        \"format\": \"json\",\n",
        "        \"utf8\": 1\n",
        "    }\n",
        "    r = requests.get(url, params=params, headers=HEADERS, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    out = []\n",
        "    for item in data.get(\"query\", {}).get(\"search\", [])[:limit]:\n",
        "        title = item[\"title\"]\n",
        "        page_url = f\"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
        "        out.append({\"source\": \"wikipedia\", \"title\": title, \"url\": page_url})\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "022fc254",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_pubmed(query: str, limit: int = 5) -> List[Dict]:\n",
        "    esearch = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    r = requests.get(esearch, params={\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": query,\n",
        "        \"retmax\": limit,\n",
        "        \"retmode\": \"json\"\n",
        "    }, headers=HEADERS, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    ids = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "    out = []\n",
        "    for pmid in ids:\n",
        "        out.append({\"source\": \"pubmed\", \"title\": f\"PubMed PMID:{pmid}\", \"url\": f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"})\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "505592db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_openalex(query: str, limit: int = 5) -> List[Dict]:\n",
        "    url = \"https://api.openalex.org/works\"\n",
        "    r = requests.get(url, params={\"search\": query, \"per_page\": limit}, headers=HEADERS, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    out = []\n",
        "    for w in data.get(\"results\", [])[:limit]:\n",
        "        title = w.get(\"title\") or \"Untitled\"\n",
        "        landing = w.get(\"primary_location\", {}).get(\"landing_page_url\") or w.get(\"id\")\n",
        "        out.append({\"source\": \"openalex\", \"title\": title, \"url\": landing})\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51d31bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_crossref(query: str, limit: int = 5) -> List[Dict]:\n",
        "    url = \"https://api.crossref.org/works\"\n",
        "    r = requests.get(url, params={\"query\": query, \"rows\": limit}, headers=HEADERS, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    items = r.json().get(\"message\", {}).get(\"items\", [])[:limit]\n",
        "    out = []\n",
        "    for it in items:\n",
        "        title = (it.get(\"title\") or [\"Untitled\"])[0]\n",
        "        link = it.get(\"URL\")\n",
        "        if not link:\n",
        "            continue\n",
        "        out.append({\"source\": \"crossref\", \"title\": title, \"url\": link})\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab174e3b",
      "metadata": {},
      "source": [
        "## Завантаження і витяг тексту"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a869bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_and_extract(url: str) -> str:\n",
        "    if not url:\n",
        "        return \"\"\n",
        "    try:\n",
        "        downloaded = trafilatura.fetch_url(url)\n",
        "        if not downloaded:\n",
        "            r = requests.get(url, headers=HEADERS, timeout=25)\n",
        "            r.raise_for_status()\n",
        "            downloaded = r.text\n",
        "        text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n",
        "        return clean_text(text or \"\")\n",
        "    except Exception:\n",
        "        return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79d4488",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Doc:\n",
        "    title: str\n",
        "    url: str\n",
        "    source: str\n",
        "    text: str\n",
        "\n",
        "def build_corpus(query: str, lang=\"uk\", per_source=3) -> List[Doc]:\n",
        "    hits = []\n",
        "    hits += search_wikipedia(query, lang=lang, limit=per_source)\n",
        "    hits += search_pubmed(query, limit=per_source)\n",
        "    hits += search_openalex(query, limit=per_source)\n",
        "    hits += search_crossref(query, limit=per_source)\n",
        "\n",
        "    docs: List[Doc] = []\n",
        "    seen = set()\n",
        "    for h in hits:\n",
        "        url = h.get(\"url\")\n",
        "        if not url or url in seen:\n",
        "            continue\n",
        "        seen.add(url)\n",
        "        text = fetch_and_extract(url)\n",
        "        if len(text) < 500:\n",
        "            continue\n",
        "        docs.append(Doc(title=h[\"title\"], url=url, source=h[\"source\"], text=text))\n",
        "        time.sleep(0.25)\n",
        "    return docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4b001e",
      "metadata": {},
      "source": [
        "## Конспект: «визначення + схема/етапи + ключові слова» (без цитування сирих фрагментів)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debd3bed",
      "metadata": {},
      "outputs": [],
      "source": [
        "UA_STOP = {\n",
        "  \"що\",\"це\",\"та\",\"і\",\"й\",\"або\",\"але\",\"в\",\"у\",\"на\",\"до\",\"від\",\"для\",\"про\",\"як\",\n",
        "  \"які\",\"яка\",\"яке\",\"якій\",\"якого\",\"якою\",\"яким\",\"якими\",\"чи\",\"не\",\"так\",\"є\",\n",
        "  \"з\",\"із\",\"за\",\"над\",\"під\",\"при\",\"між\",\"о\",\"об\",\"по\",\"коли\",\"де\",\"хто\",\"ти\",\"ви\",\n",
        "  \"мені\",\"тобі\",\"йому\",\"їй\",\"їм\",\"нас\",\"вам\",\"їх\",\"тут\",\"там\",\"цей\",\"ця\",\"це\",\"ці\"\n",
        "}\n",
        "\n",
        "def keywords_from_query(q: str) -> List[str]:\n",
        "    qn = clean_text(q).lower()\n",
        "    words = [w for w in re.split(r\"\\s+\", re.sub(r\"[^\\w\\sА-Яа-яІіЇїЄєҐґ]\", \" \", qn)) if w]\n",
        "    words = [w for w in words if len(w) >= 3 and w not in UA_STOP]\n",
        "    # унікальні, в початковому порядку\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for w in words:\n",
        "        if w not in seen:\n",
        "            out.append(w)\n",
        "            seen.add(w)\n",
        "    return out[:10]\n",
        "\n",
        "def pick_definition_and_steps(text: str) -> Tuple[str, str]:\n",
        "    # дуже проста евристика: беремо 1-2 перші «сильні» речення як визначення,\n",
        "    # і 3-6 речень/маркерів з \"етап\", \"фаза\", \"процес\", \"цикл\" як «схема/етапи».\n",
        "    t = text or \"\"\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    if not t:\n",
        "        return \"\", \"\"\n",
        "\n",
        "    sentences = re.split(r\"(?<=[.!?…])\\s+\", t)\n",
        "    sentences = [s.strip() for s in sentences if len(s.strip()) > 40]\n",
        "\n",
        "    def_score = []\n",
        "    for s in sentences[:40]:\n",
        "        score = 0\n",
        "        if re.search(r\"\\b(це|називають|визначають|є)\\b\", s, re.I):\n",
        "            score += 2\n",
        "        if len(s) <= 220:\n",
        "            score += 1\n",
        "        def_score.append((score, s))\n",
        "    def_score.sort(key=lambda x: x[0], reverse=True)\n",
        "    definition = \" \".join([x[1] for x in def_score[:2] if x[0] > 0])\n",
        "    if not definition and sentences:\n",
        "        definition = sentences[0][:260]\n",
        "\n",
        "    step_like = []\n",
        "    for s in sentences:\n",
        "        if re.search(r\"\\b(етап|фаза|стаді|цикл|послідов|процес|крок|потім|далі|спочатку)\\b\", s, re.I):\n",
        "            step_like.append(s)\n",
        "        if len(step_like) >= 6:\n",
        "            break\n",
        "    scheme = \" \".join(step_like[:6])\n",
        "\n",
        "    return definition.strip(), scheme.strip()\n",
        "\n",
        "def simple_answer(query: str, docs: List[Doc], max_chars=2200) -> str:\n",
        "    if not docs:\n",
        "        return \"Не вдалося знайти достатньо відкритих джерел під цей запит.\"\n",
        "\n",
        "    # зливаємо кілька перших документів\n",
        "    merged = \" \".join([truncate(d.text, 1200) for d in docs[:6]])\n",
        "    merged = truncate(merged, max_chars)\n",
        "\n",
        "    definition, scheme = pick_definition_and_steps(merged)\n",
        "    keys = keywords_from_query(query)\n",
        "\n",
        "    out = []\n",
        "    out.append(f\"Запит: {query}\")\n",
        "    if definition:\n",
        "        out.append(f\"\\nВизначення: {definition}\")\n",
        "    if scheme:\n",
        "        out.append(f\"\\nСхема/етапи: {scheme}\")\n",
        "    if keys:\n",
        "        out.append(\"\\nКлючові слова: \" + \", \".join(keys))\n",
        "    out.append(\"\\n\\n(Авто-конспект з відкритих джерел. За потреби можеш попросити: коротше/довше/тільки визначення/тільки етапи.)\")\n",
        "    return \"\\n\".join(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222771bb",
      "metadata": {},
      "source": [
        "## Експорт у `../data/raw/` для GitHub Pages\n",
        "\n",
        "Додаємо:\n",
        "- `web_notes.txt` (сирий корпус)\n",
        "- `web_answer.txt` (людський конспект)\n",
        "- `biology_basics_from_web.txt` (**структурований файл** під твій веб-бот: група/ключі/відповідь)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a891354b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_to_txt(docs: List[Doc], out_path: str):\n",
        "    lines = []\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        lines.append(f\"=== DOC #{i} ===\")\n",
        "        lines.append(f\"TITLE: {d.title}\")\n",
        "        lines.append(f\"SOURCE: {d.source}\")\n",
        "        lines.append(f\"URL: {d.url}\")\n",
        "        lines.append(\"\")\n",
        "        lines.append(d.text)\n",
        "        lines.append(\"\")\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "def export_answer_only(answer: str, out_path: str):\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(answer)\n",
        "\n",
        "def export_group_file(query: str, answer: str, out_path: str):\n",
        "    # Робимо 1 групу на 1 запит. Можеш накопичувати багато запитів у одному файлі (append=True)\n",
        "    keys = keywords_from_query(query)\n",
        "    group_title = f\"{query}\".strip()\n",
        "    lines = []\n",
        "    lines.append(f\"ГРУПА ЗАПИТУ: {group_title}\")\n",
        "    lines.append(\"Ключові слова: \" + \", \".join(keys))\n",
        "    lines.append(\"Відповідь:\")\n",
        "    lines.append(answer)\n",
        "    lines.append(\"\")\n",
        "    with open(out_path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dce44f8",
      "metadata": {},
      "source": [
        "## Запуск\n",
        "\n",
        "Зміни `query` і запусти. Файли збережуться у:\n",
        "- `data/raw/web_notes.txt`\n",
        "- `data/raw/web_answer.txt`\n",
        "- `data/raw/biology_basics_from_web.txt` (структуровані блоки)\n",
        "\n",
        "Потім зроби **commit+push**, щоб GitHub Pages їх підхопив."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1f0793",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"мітохондрія функції етапи утворення АТФ\"  # <-- змінюй тут\n",
        "\n",
        "docs = build_corpus(query, lang=\"uk\", per_source=3)\n",
        "print(\"Docs:\", len(docs))\n",
        "\n",
        "out_corpus = os.path.join(DATA_DIR, \"web_notes.txt\")\n",
        "export_to_txt(docs, out_corpus)\n",
        "print(\"Saved:\", out_corpus)\n",
        "\n",
        "ans = simple_answer(query, docs)\n",
        "out_ans = os.path.join(DATA_DIR, \"web_answer.txt\")\n",
        "export_answer_only(ans, out_ans)\n",
        "print(\"Saved:\", out_ans)\n",
        "\n",
        "out_group = os.path.join(DATA_DIR, \"biology_basics_from_web.txt\")\n",
        "export_group_file(query, ans, out_group)\n",
        "print(\"Saved/Append group:\", out_group)\n",
        "\n",
        "print(\"\\n--- ANSWER PREVIEW ---\\n\")\n",
        "print(ans[:1400])\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
