{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚úÇÔ∏è Chunking Demo (extended)\n",
        "\n",
        "–¶–µ–π –Ω–æ—É—Ç–±—É–∫ –ø–æ–∫–∞–∑—É—î, —è–∫ –ø—Ä–∞—Ü—é—î —á–∞–Ω–∫—ñ–Ω–≥ (—Ä–æ–∑–±–∏—Ç—Ç—è —Ç–µ–∫—Å—Ç—É –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏) –ø–µ—Ä–µ–¥ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—î—é RAG.\n",
        "\n",
        "**–©–æ —Ä–æ–±–∏–º–æ:**\n",
        "- –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ñ–∞–π–ª–∏ –∑ `data/raw/`\n",
        "- —Ä–æ–∑–±–∏–≤–∞—î–º–æ —Ç–µ–∫—Å—Ç–∏ –Ω–∞ —á–∞–Ω–∫–∏ –∑ overlap\n",
        "- –≤–∏–≤–æ–¥–∏–º–æ –ø–µ—Ä—à—ñ —á–∞–Ω–∫–∏ + –º–µ—Ç–∞–¥–∞–Ω—ñ\n",
        "- –∞–Ω–∞–ª—ñ–∑—É—î–º–æ —Ä–æ–∑–º—ñ—Ä–∏ —á–∞–Ω–∫—ñ–≤ —ñ overlap\n",
        "- –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–º—ñ—Ç—Ç—è —Ç–∏–ø—É `=====` —É —á–∞–Ω–∫–∞—Ö\n",
        "- —Ç–µ—Å—Ç—É—î–º–æ –ø—Ä–æ—Å—Ç–∏–π –ø–æ—à—É–∫ –ø–æ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª–æ–≤–∞—Ö (lexical) –¥–ª—è sanity-check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –Ø–∫—â–æ –∑–∞–ø—É—Å–∫–∞—î—à –Ω–æ—É—Ç–±—É–∫ –∑ –ø–∞–ø–∫–∏ notebooks/, —Ç—Ä–µ–±–∞ –¥–æ–¥–∞—Ç–∏ –∫–æ—Ä—ñ–Ω—å –ø—Ä–æ—î–∫—Ç—É –≤ sys.path\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path('..').resolve()\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "print('Project root:', ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data_loader import load_raw_docs\n",
        "from src.chunking import chunk_docs\n",
        "\n",
        "docs = load_raw_docs('data/raw')\n",
        "chunks = chunk_docs(docs, chunk_size=300, chunk_overlap=60)\n",
        "\n",
        "print(f'Documents loaded: {len(docs)}')\n",
        "print(f'Chunks created: {len(chunks)}')\n",
        "print('Example doc meta:', getattr(docs[0], 'meta', None) if docs else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–¥–∏–≤–∏–º–æ—Å—å –ø–µ—Ä—à—ñ 5 —á–∞–Ω–∫—ñ–≤\n",
        "for i, ch in enumerate(chunks[:5], start=1):\n",
        "    print('\\n' + '='*60)\n",
        "    print(f'Chunk #{i}')\n",
        "    print('Meta:', ch.meta)\n",
        "    print('-'*60)\n",
        "    print(ch.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìè –ê–Ω–∞–ª—ñ–∑ —á–∞–Ω–∫—ñ–≤\n",
        "\n",
        "–ü–æ–¥–∏–≤–∏–º–æ—Å—å:\n",
        "- –¥–æ–≤–∂–∏–Ω–∏ —á–∞–Ω–∫—ñ–≤\n",
        "- —Ä–æ–∑–ø–æ–¥—ñ–ª –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
        "- —á–∏ —î –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ/–¥—É–∂–µ –¥–æ–≤–≥—ñ —á–∞–Ω–∫–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def chunk_len(ch):\n",
        "    return len(ch.text or '')\n",
        "\n",
        "lens = [chunk_len(c) for c in chunks]\n",
        "if lens:\n",
        "    print('Min:', min(lens), 'Max:', max(lens), 'Avg:', round(sum(lens)/len(lens), 1))\n",
        "\n",
        "    # buckets\n",
        "    buckets = Counter()\n",
        "    for L in lens:\n",
        "        if L < 120: buckets['<120'] += 1\n",
        "        elif L < 200: buckets['120-199'] += 1\n",
        "        elif L < 300: buckets['200-299'] += 1\n",
        "        elif L < 420: buckets['300-419'] += 1\n",
        "        else: buckets['>=420'] += 1\n",
        "    print('Length buckets:', dict(buckets))\n",
        "else:\n",
        "    print('No chunks')\n",
        "\n",
        "# chunks per source\n",
        "by_src = defaultdict(int)\n",
        "for c in chunks:\n",
        "    src = (c.meta or {}).get('source', 'unknown')\n",
        "    by_src[src] += 1\n",
        "\n",
        "print('\\nChunks per source (top 20):')\n",
        "for src, n in sorted(by_src.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
        "    print(f'  {src}: {n}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÅ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ overlap (–Ω–∞ —Ä—ñ–≤–Ω—ñ —Ç–µ–∫—Å—Ç—É)\n",
        "\n",
        "–ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ: —á–∏ —Ä–µ–∞–ª—å–Ω–æ —Å—É—Å—ñ–¥–Ω—ñ —á–∞–Ω–∫–∏ –º–∞—é—Ç—å —Å–ø—ñ–ª—å–Ω–∏–π ¬´—Ö–≤—ñ—Å—Ç/–≥–æ–ª–æ–≤—É¬ª –ø—Ä–∏–±–ª–∏–∑–Ω–æ –Ω–∞ `chunk_overlap` —Å–∏–º–≤–æ–ª—ñ–≤.\n",
        "\n",
        "> –¶–µ sanity-check, –±–æ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è overlap –º–æ–∂–µ –±—É—Ç–∏ –ø–æ —Å–∏–º–≤–æ–ª–∞—Ö/—Å–ª–æ–≤–∞—Ö ‚Äî –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ `src.chunking`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def common_prefix_len(a: str, b: str, max_check=200):\n",
        "    a = a or ''\n",
        "    b = b or ''\n",
        "    m = min(len(a), len(b), max_check)\n",
        "    i = 0\n",
        "    while i < m and a[i] == b[i]:\n",
        "        i += 1\n",
        "    return i\n",
        "\n",
        "def common_suffix_prefix(suffix_from: str, prefix_from: str, max_check=200):\n",
        "    s = suffix_from or ''\n",
        "    p = prefix_from or ''\n",
        "    m = min(len(s), len(p), max_check)\n",
        "    # check from 1..m the largest k such that s[-k:] == p[:k]\n",
        "    best = 0\n",
        "    for k in range(1, m+1):\n",
        "        if s[-k:] == p[:k]:\n",
        "            best = k\n",
        "    return best\n",
        "\n",
        "# group by source and sort by start index if available\n",
        "by_source = defaultdict(list)\n",
        "for c in chunks:\n",
        "    src = (c.meta or {}).get('source', 'unknown')\n",
        "    by_source[src].append(c)\n",
        "\n",
        "def start_pos(c):\n",
        "    return (c.meta or {}).get('start', 0)\n",
        "\n",
        "samples = 0\n",
        "for src, arr in by_source.items():\n",
        "    arr = sorted(arr, key=start_pos)\n",
        "    for i in range(len(arr)-1):\n",
        "        a, b = arr[i], arr[i+1]\n",
        "        ov = common_suffix_prefix(a.text, b.text, max_check=250)\n",
        "        if ov > 0:\n",
        "            print(f\"{src}: chunk {i} -> {i+1} overlap_like={ov}\")\n",
        "            samples += 1\n",
        "        if samples >= 15:\n",
        "            break\n",
        "    if samples >= 15:\n",
        "        break\n",
        "\n",
        "if samples == 0:\n",
        "    print('No detectable string overlap in first samples (–º–æ–∂–µ –±—É—Ç–∏ —Ç–æ–∫–µ–Ω–Ω–∏–π/—Å–ª–æ–≤–Ω–∏–π overlap –∞–±–æ —ñ–Ω—à–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßΩ –ü–æ—à—É–∫ ¬´—Å–º—ñ—Ç—Ç—è¬ª –≤ —á–∞–Ω–∫–∞—Ö\n",
        "\n",
        "–ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –ø—Ä–∏—Å—É—Ç–Ω—ñ –ª—ñ–Ω—ñ—ó —Ç–∏–ø—É:\n",
        "- `=====`\n",
        "- `-----`\n",
        "- `____`\n",
        "- `***`\n",
        "\n",
        "–¢–∞–∫–µ —Å–º—ñ—Ç—Ç—è –ø–æ—Ç—ñ–º –º–æ–∂–µ ¬´–≤–∏–ª–∞–∑–∏—Ç–∏¬ª –≤ –≤—ñ–¥–ø–æ–≤—ñ–¥—è—Ö."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEP_RE = re.compile(r\"^(={3,}|-{3,}|_{3,}|\\*{3,})\\s*$\", re.MULTILINE)\n",
        "\n",
        "bad_chunks = []\n",
        "for idx, ch in enumerate(chunks):\n",
        "    if SEP_RE.search(ch.text or ''):\n",
        "        bad_chunks.append(idx)\n",
        "\n",
        "print('Chunks with separators:', len(bad_chunks))\n",
        "print('First indexes:', bad_chunks[:20])\n",
        "\n",
        "if bad_chunks:\n",
        "    i = bad_chunks[0]\n",
        "    print('\\nExample bad chunk index:', i)\n",
        "    print('Meta:', chunks[i].meta)\n",
        "    print(chunks[i].text[:700])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîé –ü—Ä–æ—Å—Ç–∏–π lexical-search –ø–æ —á–∞–Ω–∫–∞—Ö (sanity check)\n",
        "\n",
        "–¶–µ –Ω–µ –∑–∞–º—ñ–Ω–∞ –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤/RAG, –∞–ª–µ –¥–æ–ø–æ–º–∞–≥–∞—î:\n",
        "- —à–≤–∏–¥–∫–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —á–∏ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ —Ä–µ–∞–ª—å–Ω–æ –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –≤ —á–∞–Ω–∫–∞—Ö\n",
        "- –ø–æ–¥–∏–≤–∏—Ç–∏—Å—å —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UA_STOP = {\n",
        "  \"—â–æ\",\"—Ü–µ\",\"—Ç–∞\",\"—ñ\",\"–π\",\"–∞–±–æ\",\"–∞–ª–µ\",\"–≤\",\"—É\",\"–Ω–∞\",\"–¥–æ\",\"–≤—ñ–¥\",\"–¥–ª—è\",\"–ø—Ä–æ\",\"—è–∫\",\n",
        "  \"—è–∫—ñ\",\"—è–∫–∞\",\"—è–∫–µ\",\"—è–∫—ñ–π\",\"—è–∫–æ–≥–æ\",\"—è–∫–æ—é\",\"—è–∫–∏–º\",\"—è–∫–∏–º–∏\",\"—á–∏\",\"–Ω–µ\",\"—Ç–∞–∫\",\"—î\",\n",
        "  \"–∑\",\"—ñ–∑\",\"–∑–∞\",\"–Ω–∞–¥\",\"–ø—ñ–¥\",\"–ø—Ä–∏\",\"–º—ñ–∂\",\"–æ\",\"–æ–±\",\"–ø–æ\",\"–∫–æ–ª–∏\",\"–¥–µ\",\"—Ö—Ç–æ\",\"—Ç–∏\",\"–≤–∏\",\n",
        "  \"–º–µ–Ω—ñ\",\"—Ç–æ–±—ñ\",\"–π–æ–º—É\",\"—ó–π\",\"—ó–º\",\"–Ω–∞—Å\",\"–≤–∞–º\",\"—ó—Ö\",\"—Ç—É—Ç\",\"—Ç–∞–º\",\"—Ü–µ–π\",\"—Ü—è\",\"—Ü–µ\",\"—Ü—ñ\"\n",
        "}\n",
        "\n",
        "def normalize(s: str):\n",
        "    s = (s or '').lower()\n",
        "    s = s.replace('‚Äô', \"'\").replace('`', \"'\")\n",
        "    s = re.sub(r\"[^\\w\\s'–ê-–Ø–∞-—è–Ü—ñ–á—ó–Ñ—î“ê“ë]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def tokens(s: str):\n",
        "    t = normalize(s)\n",
        "    out = []\n",
        "    for w in t.split():\n",
        "        if len(w) >= 3 and w not in UA_STOP:\n",
        "            out.append(w)\n",
        "    return out\n",
        "\n",
        "def lexical_score(query: str, text: str):\n",
        "    q = tokens(query)\n",
        "    if not q:\n",
        "        return 0\n",
        "    tt = normalize(text)\n",
        "    score = 0\n",
        "    for w in q:\n",
        "        if w in tt:\n",
        "            score += 1\n",
        "    return score\n",
        "\n",
        "def search_chunks(query: str, topk=5):\n",
        "    scored = []\n",
        "    for i, ch in enumerate(chunks):\n",
        "        s = lexical_score(query, ch.text or '')\n",
        "        if s > 0:\n",
        "            scored.append((s, i))\n",
        "    scored.sort(reverse=True)\n",
        "    return scored[:topk]\n",
        "\n",
        "query = \"—Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—è –î–ù–ö\"\n",
        "hits = search_chunks(query, topk=5)\n",
        "print('Query:', query)\n",
        "print('Hits:', hits)\n",
        "for rank, (s, idx) in enumerate(hits, 1):\n",
        "    ch = chunks[idx]\n",
        "    print('\\n' + '='*60)\n",
        "    print(f'Rank {rank} | score={s} | idx={idx}')\n",
        "    print('Meta:', ch.meta)\n",
        "    print('-'*60)\n",
        "    print((ch.text or '')[:900])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ –û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "\n",
        "- `Documents loaded` –º–∞—î –±—É—Ç–∏ > 0 (—è–∫—â–æ –≤ `data/raw` —î —Ñ–∞–π–ª–∏)\n",
        "- `Chunks created` —Ç–µ–∂ > 0\n",
        "- –£ –≤–∏–≤–æ–¥—ñ –≤–∏–¥–Ω–æ `meta` (source/start/end) —ñ —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞\n",
        "- –í –∞–Ω–∞–ª—ñ–∑—ñ –≤–∏–¥–Ω–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –¥–æ–≤–∂–∏–Ω —ñ –ø—ñ–¥–æ–∑—Ä—ñ–ª—ñ —á–∞–Ω–∫–∏\n",
        "- Lexical search –¥–∞—î –∫—ñ–ª—å–∫–∞ —á–∞–Ω–∫—ñ–≤, –¥–µ —Ä–µ–∞–ª—å–Ω–æ —î –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
